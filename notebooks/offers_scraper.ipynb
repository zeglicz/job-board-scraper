{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offers Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dotenv import dotenv_values  # type: ignore\n",
    "\n",
    "import requests  # type: ignore\n",
    "from requests.adapters import HTTPAdapter, Retry  # type: ignore\n",
    "from requests_cache import CachedSession  # type: ignore\n",
    "from requests.exceptions import RetryError, RequestException  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = dotenv_values(\"../.env\")\n",
    "OFFERS_URL = env.get(\"OFFERS_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobOffersScraper:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_url: str,\n",
    "        cache_name=\"fetcher_cache\",\n",
    "        cache_expire=60,\n",
    "        use_cache=False,\n",
    "        min_delay=1,\n",
    "        max_delay=10,\n",
    "        retries=5,\n",
    "        backoff_range=(1, 7),\n",
    "        save_dir=\"../data/raw/json\"\n",
    "    ):\n",
    "        if not base_url:\n",
    "            raise ValueError(\"URL is required and cannot be empty\")\n",
    "        self.base_url = base_url\n",
    "        self.min_delay = min_delay\n",
    "        self.max_delay = max_delay\n",
    "        self.retries = retries\n",
    "        self.backoff_range = backoff_range\n",
    "        self.session = self._init_session(cache_name, cache_expire, use_cache)\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "    def _init_session(self, cache_name, cache_expire, use_cache):\n",
    "        backoff_factor = random.uniform(*self.backoff_range)\n",
    "\n",
    "        retry_strategy = Retry(\n",
    "            total=self.retries,\n",
    "            backoff_factor=backoff_factor,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"GET\"],\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "\n",
    "        if use_cache:\n",
    "            session = CachedSession(\n",
    "                cache_name,\n",
    "                expire_after=cache_expire,\n",
    "                use_cache=True\n",
    "            )\n",
    "        else:\n",
    "            session = requests.Session()\n",
    "\n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "\n",
    "        def log_response_hook(response, *args, **kwargs):\n",
    "            logger.info(f\"Hook: {response.status_code} {response.url}\")\n",
    "\n",
    "        session.hooks[\"response\"] = [log_response_hook]\n",
    "\n",
    "        return session\n",
    "\n",
    "    def save_json(self, data: dict, page: int):\n",
    "        Path(self.save_dir).mkdir(parents=True, exist_ok=True)\n",
    "        with open(f\"{self.save_dir}/offers_page_{page}.json\", \"w\") as f:\n",
    "            f.write(json.dumps(data, indent=4))\n",
    "\n",
    "    def fetch_page(self, page: int = 1) -> dict:\n",
    "        try:\n",
    "            response = self.session.get(self.base_url, params={\"page\": page})\n",
    "            response.raise_for_status()\n",
    "            logger.info(f\"Download page: {page}\")\n",
    "            self.save_json(response.json(), page)\n",
    "\n",
    "            return response.json()\n",
    "        except RetryError as e:\n",
    "            raise RuntimeError(f\"All retries failed: {e}\")\n",
    "        except RequestException as e:\n",
    "            logger.error(f\"Request failed: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: {e}\")\n",
    "            raise\n",
    "\n",
    "    def fetch_pages(self, max_pages: int = 5):\n",
    "        pages = list(range(1, max_pages + 1))\n",
    "        random.shuffle(pages)\n",
    "\n",
    "        for page in pages:\n",
    "            self.fetch_page(page)\n",
    "            logger.info(f\"Saved page: {page}\")\n",
    "            time.sleep(random.randint(self.min_delay, self.max_delay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "path_dir = f\"../data/raw/{today_str}\"\n",
    "\n",
    "job_offers_fetcher = JobOffersScraper(OFFERS_URL, save_dir=path_dir)\n",
    "job_offers_fetcher.fetch_pages(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "job-board-scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
